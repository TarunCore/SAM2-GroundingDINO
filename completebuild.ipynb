{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "%cd {HOME}/GroundingDINO\n",
    "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -v -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
    "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from groundingdino.util.inference import Model\n",
    "\n",
    "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SAM_ENCODER_VERSION = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import supervision as sv\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "model = load_model(GROUNDING_DINO_CONFIG_PATH, GROUNDING_DINO_CHECKPOINT_PATH)\n",
    "\n",
    "IMAGE_NAME = \"dog-4.jpeg\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
    "\n",
    "TEXT_PROMPT = \"bag\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "# Convert GroundingDINO boxes to NumPy array\n",
    "xyxy = boxes.cpu().numpy()  # Convert tensor to numpy array\n",
    "confidence = logits.cpu().numpy()  # Convert logits to numpy array\n",
    "\n",
    "# Create a class_id mapping (optional)\n",
    "class_id_map = {phrase: idx for idx, phrase in enumerate(set(phrases))}\n",
    "class_ids = np.array([class_id_map[phrase] for phrase in phrases], dtype=object)\n",
    "\n",
    "# Create a Supervision Detections object\n",
    "detections2 = sv.Detections(\n",
    "    xyxy=xyxy,\n",
    "    confidence=confidence,\n",
    "    class_id=class_ids\n",
    ")\n",
    "\n",
    "# Annotate the image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Display the annotated image\n",
    "%matplotlib inline\n",
    "sv.plot_image(annotated_frame, (16, 16))\n",
    "\n",
    "# Print the detections\n",
    "print(detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "# Set the image for SAM\n",
    "sam_predictor.set_image(image_source)\n",
    "\n",
    "# Get image dimensions\n",
    "H, W, _ = image_source.shape\n",
    "\n",
    "# Convert GroundingDINO boxes to SAM input format\n",
    "boxes_scaled = boxes.clone().cpu()\n",
    "\n",
    "# GroundingDINO gives [center_x, center_y, width, height]\n",
    "# Convert to [x1, y1, x2, y2] format\n",
    "boxes_xyxy = torch.zeros_like(boxes_scaled)\n",
    "boxes_xyxy[:, 0] = boxes_scaled[:, 0] - boxes_scaled[:, 2] / 2  # x1 = center_x - width/2\n",
    "boxes_xyxy[:, 1] = boxes_scaled[:, 1] - boxes_scaled[:, 3] / 2  # y1 = center_y - height/2\n",
    "boxes_xyxy[:, 2] = boxes_scaled[:, 0] + boxes_scaled[:, 2] / 2  # x2 = center_x + width/2\n",
    "boxes_xyxy[:, 3] = boxes_scaled[:, 1] + boxes_scaled[:, 3] / 2  # y2 = center_y + height/2\n",
    "\n",
    "# Scale the normalized coordinates to pixel coordinates\n",
    "boxes_xyxy[:, [0, 2]] *= W  # scale x coordinates\n",
    "boxes_xyxy[:, [1, 3]] *= H  # scale y coordinates\n",
    "\n",
    "# Convert to numpy array\n",
    "boxes_scaled = boxes_xyxy.numpy()\n",
    "\n",
    "# Print the coordinates for debugging\n",
    "print(\"Original normalized coordinates:\", boxes.cpu().numpy())\n",
    "print(\"Converted to [x1,y1,x2,y2] format:\", boxes_xyxy.numpy())\n",
    "print(\"Final scaled pixel coordinates:\", boxes_scaled)\n",
    "\n",
    "# Get masks for all detected boxes\n",
    "masks_list = []\n",
    "scores_list = []\n",
    "for box in boxes_scaled:\n",
    "    # Convert box to format expected by SAM\n",
    "    sam_box = box.astype(int)\n",
    "\n",
    "    # Get mask prediction from SAM\n",
    "    masks, scores, logits = sam_predictor.predict(\n",
    "        box=sam_box,\n",
    "        multimask_output=False\n",
    "    )\n",
    "\n",
    "    masks_list.append(masks[0])\n",
    "    scores_list.append(scores[0])\n",
    "\n",
    "# Convert masks to a single numpy array\n",
    "masks = np.stack(masks_list, axis=0)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0, x1, y1 = box\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), x1 - x0, y1 - y0,\n",
    "                              edgecolor='red', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "# Create an empty combined mask (same size as individual masks)\n",
    "combined_mask = np.zeros_like(masks[0], dtype=np.uint8)\n",
    "\n",
    "# Apply bitwise OR for each mask\n",
    "for mask in masks:\n",
    "    mask_uint8 = (mask.astype(np.uint8)) * 255  # Convert boolean to uint8 (255 for True, 0 for False)\n",
    "    combined_mask = cv2.bitwise_or(combined_mask, mask_uint8)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# **Left: Original Image with Bounding Boxes**\n",
    "axes[0].imshow(image_source)\n",
    "for box in boxes_scaled:\n",
    "    show_box(box, axes[0])  # Draw bounding boxes\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Original Image with Bounding Boxes\")\n",
    "\n",
    "# **Right: Combined Mask**\n",
    "axes[1].imshow(combined_mask, cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Combined Mask\")\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
